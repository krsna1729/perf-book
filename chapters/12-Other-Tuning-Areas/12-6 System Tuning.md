## System Tuning {#sec:SysTune}

After completing all the hard work of tuning an application to exploit all the intricate facilities of the CPU microarchitecture, the last thing we want is for the system firmware, the OS, or the kernel to destroy all our efforts. The most highly tuned application will mean very little if it is intermittently disrupted by a system interrupt that halts the entire system. Such interrupt might run for up to tens to hundreds of milliseconds at a time.

Developers usually have little to no control over the environment in which the application is executed. When we ship the product, it's unrealistic to tune every setup a customer might have. Usually, large enough organizations have separate Operations Teams (Ops), which handle such sort of issues. It is in our interest to provide them with recommendations on how to set up the system to get the best performance out of our application.

There are many things to tune in a modern system, and avoiding system-based interference is not an easy task. An example of a performance tuning manual of x86-based server deployments is Red Hat [guidelines](https://access.redhat.com/sites/default/files/attachments/201501-perf-brief-low-latency-tuning-rhel7-v2.1.pdf)[^5]. There, you will find tips for eliminating or significantly minimizing cache-disrupting interrupts from sources like the system BIOS, the Linux kernel, and from device drivers, among many other sources of application interference. These guidelines should serve as a baseline image for all new server builds before any application is deployed into a production environment.

Most out-of-the-box platforms are configured for optimal throughput while saving power when possible. But there are industries with real-time requirements, which care more about having lower latency than everything else. An example of such an industry can be robots operating in automotive assembly lines. Actions performed by such robots are triggered by external events and usually have a predetermined time budget to finish because the next interrupt will come shortly (it is usually called a "control loop"). Meeting real-time goals for such a platform may require sacrificing the overall throughput of the machine or allowing it to consume more energy. One of the popular techniques in that area is to disable processor sleeping states[^7] to keep it ready to react immediately. Another interesting technique is called Cache Locking,[^8] where portions of the CPU cache are reserved for a particular set of data. It helps to streamline the memory latencies within an application.

A more extreme shot at boosting performance is to overclock the CPU. Overclocking is a process of running the CPU at a higher frequency than it was designed for. It is a risky operation, as it can void the warranty and potentially damage the CPU. Overclocking is not suited for production environments and is usually done by enthusiasts who are willing to take the risk for the sake of performance. To overclock a CPU, you need to have the right parts, mainly a motherboard that supports overclocking, a CPU that has unlocked clock frequency, and a cooling system that can handle the increased heat output. At the beginning of 2024, overclocking experts crossed the 9 GHz barrier on a widely available CPU.[^9]

It is helpful to understand performance bottlenecks in your application to tune the right settings. Scalability studies can help you to determine the sensitivity of your application to various system settings. For example, you may find out that your application doesn't scale with the number of cores (see [@sec:ThreadCountScalingStudy]), or that it is limited by the memory latency. Using this information, you can make educated decisions about tuning the system settings or perhaps buying new hardware components for your computing systems. In the next case study, we will show how you can determine whether an application is sensitive to the size of the last-level cache (LLC).

[^5]: Red Hat low latency tuning guidelines - [https://access.redhat.com/sites/default/files/attachments/201501-perf-brief-low-latency-tuning-rhel7-v2.1.pdf](https://access.redhat.com/sites/default/files/attachments/201501-perf-brief-low-latency-tuning-rhel7-v2.1.pdf)
[^7]: Power Management States: P-States, C-States - [https://software.intel.com/content/www/us/en/develop/articles/power-management-states-p-states-c-states-and-package-c-states.html](https://software.intel.com/content/www/us/en/develop/articles/power-management-states-p-states-c-states-and-package-c-states.html)
[^8]: Cache Locking. Survey of cache locking techniques [@CacheLocking]. An example of pseudo-locking a portion of the cache, which is then exposed as a character device in the Linux file system and made available for `mmap`ing: [https://events19.linuxfoundation.org/wp-content/uploads/2017/11/Introducing-Cache-Pseudo-Locking-to-Reduce-Memory-Access-Latency-Reinette-Chatre-Intel.pdf](https://events19.linuxfoundation.org/wp-content/uploads/2017/11/Introducing-Cache-Pseudo-Locking-to-Reduce-Memory-Access-Latency-Reinette-Chatre-Intel.pdf)
[^9]: CPU overclocking records - [https://press.asus.com/news/press-releases/rog-maximus-z790-apex-encore-sets-3-overclocking-world-records/](https://press.asus.com/news/press-releases/rog-maximus-z790-apex-encore-sets-3-overclocking-world-records/)
