## CPU-Specific Optimizations

Optimizing software for a specific CPU microarchitecture involves tailoring your code to leverage the strengths and mitigate the weaknesses of that microarchitecture. It is easier to do when you know the exact target CPU for your application. However, most applications run on a wide range of CPUs. Optimizing the performance of a cross-platform application with very high-speed requirements can be challenging since platforms from different vendors have different designs and implementations. Nevertheless, it is possible to write code that performs reasonably well on CPUs from different vendors, while providing a fine-tuned version for a specific microarchitecture. 

The major differences between x86 (considered as CISC) and RISC ISAs, such as ARM and RISC-V, are summarized below:

* x86 instructions are variable-length, while ARM and RISC-V instructions are fixed-length. This makes decoding x86 instructions more complex.
* x86 ISA has many addressing modes, while ARM and RISC-V have few addressing modes. Operands in ARM and RISC-V instructions are either registers or immediate values, while x86 instruction inputs can also come from memory. This bloats the number of x86 instructions but also allows for more powerful single instructions. For instance, ARM requires loading a memory location first, then performing the operation; x86 can do both in one instruction.

In addition to this, there are a few other differences that you should consider when optimizing for a specific microarchitecture. As of 2024, the most recent x86-64 ISA has 16 architectural general-purpose registers, while the latest ARMv8 and RV64 require a CPU to provide 32 general-purpose registers. Extra architectural registers reduce register spilling and hence reduce the number of loads/stores. Intel has announced a new extension called APX[^1] that will increase the number of registers to 32.

There is also a difference in the memory page size between x86 and ARM. The default page size for x86 platforms is 4 KB, while most ARM systems (for example, Apple MacBooks) use a 16 KB page size, although both platforms support larger page sizes (see [@sec:ArchHugePages], and [@sec:secDTLB]). All these differences can affect the performance of your application when they become a bottleneck.

Although ISA differences *may* have a tangible impact on the performance of a specific application, numerous studies show that on average, differences between the two most popular ISAs, namely x86 and ARM, don't have a measurable performance impact. Throughout this book, I carefully avoided advertisements of any products (e.g., Intel vs. AMD vs. Apple) and any religious ISA debates (x86 vs. ARM vs. RISC-V).[^5] Below are some references that I hope will close the debate:

* Performance or energy consumption differences are not generated by ISA differences, but rather by microarchitecture implementation. [@RISCvsCISC2013]
* ISA doesn't have a large effect on the number and type of executed instructions. [@RISCVvsAArch642023] [@RISCvsCISC2013]
* CISC code is not denser than RISC code. [@CodeDensityCISCvsRISC]
* ISA overheads can be effectively mitigated by microarchitecture implementation. For example, $\mu$op cache minimizes decoding overheads; instruction cache minimizes code density impact. [@RISCvsCISC2013] [@ChipsAndCheesex86]

Nevertheless, this doesn't remove the value of architecture-specific optimizations. In this section, we will discuss how to optimize for a particular platform. We will cover ISA extensions, the CPU dispatch technique, and discuss how to reason about instruction latencies and throughput.

### ISA Extensions

ISA evolution has been continuous. It has focused on accelerating specialized workloads, such as cryptography, AI, multimedia, and others. Utilizing ISA extensions often results in substantial performance improvements. Developers keep finding smart ways to leverage these extensions in general-purpose applications. So, even if you're outside of one of these highly specialized domains, you might still benefit from using ISA extensions.

It's not possible to learn about all specific instructions. But I suggest you familiarize yourself with major ISA extensions available on your target platform. For example, if you are developing an AI application that uses `fp16` (16-bit half-precision floating-point) data types, and you target one of the modern ARM processors, make sure that your program's machine code contains corresponding `fp16` ISA extensions. If you're developing encryption/decryption software, check if it utilizes crypto extensions of your target ISA. And so on.

Here is a list of some notable x86 ISA extensions:

* SSE/AVX/AVX2: provide SIMD instructions for floating-point and integer operations.
* AVX512: extends AVX2 with 512-bit registers and many new instructions.
* AVX512_FP16/AVX512_BF16: add support for 16-bit half-precision and `Bfloat16` floating-point values.
* AES/SHA: provide instructions for AES encryption, decryption, and SHA hashing.
* BMI/BMI2: provide instructions for bit manipulation.
* AVX_VNNI/AVX512_VNNI: Vector Neural Network Instructions for accelerating deep learning workloads.
* AMX: Advanced Matrix Extensions for accelerating matrix multiplication.

Here is a list of some notable ARM ISA extensions:

* Advanced SIMD: also known as NEON, provides arithmetic SIMD instructions.
* Cryptographic Instructions: provide instructions for encryption, hashing, and checksumming.
* FP16/BF16: provide 16-bit half-precision and `Bfloat16` floating-point instructions.
* UDOT/SDOT: support for dot product instructions for accelerating machine learning workloads.
* SVE: enables scalable vector length instructions.
* SME: Scalable Matrix Extension for accelerating matrix multiplication.

When compiling your applications, make sure to enable the necessary compiler flags to activate required ISA extensions. On GCC and Clang compilers use the `-march` option. For example, `-march=native` will activate ISA features of your host system, i.e., on which you run the compilation. Or you can include a specific version of ISA, e.g., `-march=armv8.6-a`. On the MSVC compiler, use the `/arch` option, e.g., `/arch:AVX2`.

I do not recommend using `-march=native` for production builds, because code generation will depend on which machine you're building your code. Many CI/CD systems have old machines. Building software on one of these  machines with `-march=native` may lead to suboptimal performance when the application is run on a newer machine. Instead, use `-march` with a specific microarchitecture that you want to target. 

### CPU Dispatch

When you want to provide a fast path for a specific microarchitecture while keeping a generic implementation for other platforms, you can use *CPU dispatching*. It is a technique that allows your program to detect which features your processor has, and based on that decide which version of the code to execute. It enables you to introduce platform-specific optimizations in a single codebase. As a rule of thumb, it is better to start with a generic implementation and then introduce microarchitecture-specific optimizations progressively, ensuring there is a fallback for architectures that do not have the required features. For example:

```cpp
if (__builtin_cpu_supports ("avx512f")) {
  avx512_impl();
} else {
  generic_impl();
}
```

This demonstrates the use of built-in functions that are available in GCC and Clang compilers. Besides detecting supported ISA extensions, there is a `__builtin_cpu_is` function to detect an exact processor model. A compiler-agnostic way of writing CPU dispatch is to use the `CPUID` instruction (x86-only), `getauxval(AT_HWCAP)` Linux system call, or `sysctlbyname` on macOS.

You would typically see CPU dispatching constructs used to optimize only specific parts of the code, e.g., hot function or loop. Very often, these platform-specific implementations are written with compiler intrinsics (see [@sec:secIntrinsics]) to generate desired instructions.

Even though CPU dispatching is a runtime check, its overhead is not high. You can identify hardware capabilities at startup once and save it in some variable, so at runtime, it becomes just a single branch, which is well-predicted. Perhaps a bigger concern about CPU dispatching is the maintenance cost. Every new specialized branch requires fine-tuning and validation.

### Instruction Latencies and Throughput

Besides ISA extensions, it's worth learning about the number and type of execution units in your processor (e.g., the number of loads, stores, divisions, and multiplications a processor can issue every cycle). For most processors, this information is published by CPU vendors in corresponding technical manuals. However, information about latencies and throughput of specific instructions is not usually disclosed. Nevertheless, people have benchmarked individual instructions, which can be accessed online. For the latest Intel and AMD CPUs, latency, throughput, port usage, and the number of $\mu$ops for an instruction can be found at the [uops.info](https://uops.info/table.html)[^2] website. For Apple processors, similar data is accessible in [@AppleOptimizationGuide, Appendix A].[^6] Along with instruction latencies and throughput, developers have reverse-engineered other aspects of a microarchitecture such as the size of branch prediction history buffers, reorder buffer capacity, size of load/store buffers, and others.

Be very careful about making conclusions just on the instruction latency and throughput numbers. In many cases, instruction latencies are hidden by the out-of-order execution engine, and it may not matter if an instruction has a latency of 4 or 8 cycles. If it doesn't block forward progress, such instruction will be handled "in the background" without harming performance. However, the latency of an instruction becomes important when it stands on a critical dependency chain because it delays the execution of dependent operations.

In contrast, if you have a loop that performs a lot of _independent_ operations, you should focus on instruction throughput rather than latency. When operations are independent, they can be processed in parallel. In such a scenario, the critical factor is how many operations of a certain type can be executed per cycle, or *execution throughput*. There are also "in-between" scenarios, where both instruction latency and throughput may affect performance.

When you analyze machine code for one of your hot loops, you may find that multiple instructions are assigned to the same execution port. This situation is known as _execution port contention_. So the challenge is to find ways of substituting some of these instructions with the ones that are not assigned to the critical port. For example on Intel processors, if you're heavily bottlenecked on `port5`, then you may find that two instructions on `port0` are better than one instruction on `port5`. Often it is not an easy task and it requires deep ISA and microarchitecture knowledge. When in doubt, seek help on specialized forums. Also, keep in mind that some of these things may change in future CPU generations, so consider using CPU dispatch to isolate the effect of your code changes.

### Case Study: When FMA Instructions Hurt Performance {.unlisted .unnumbered}

In [@sec:FMAThroughput], we looked at one example, of when the throughput of FMA instructions becomes critical. Now let's take a look at another example, involving FMA latency. In [@lst:FMAlatency] on the left, we have the `sqSum` function which computes a sum of every element squared. On the right, we present the corresponding machine code generated by Clang-18 when compiled with `-O3 -march=core-avx2`. Notice, that we didn't use `-ffast-math`, perhaps because we want to maintain bit-exact results over multiple platforms. That's why the code was not autovectorized by the compiler.

Listing: FMA latency

~~~~ {#lst:FMAlatency .cpp .numberLines}
float sqSum(float *a, int N) {         │ .loop:
  float sum = 0;                       │  vmovss xmm1, dword ptr [rcx + 4*rdx]
  for (int i = 0; i < N; i++ )         │  vfmadd231ss xmm0, xmm1, xmm1
    sum += a[i] * a[i];                │  inc rdx
  return sum;                          │  cmp rax, rdx
}                                      │  jne .loop
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On each iteration of the loop, we have two operations: calculate the squared value of `a[i]` and accumulate the product in the `sum` variable. If you look closer, you may notice that multiplications are independent of each other, so they can be executed in parallel. The generated machine code (on the right) uses fused multiply-add (FMA) to perform both operations with a single instruction. The problem here is that by using FMAs, the compiler has included multiplication into the critical dependency chain of the loop.

The `vfmadd231ss` instruction computes the squared value of `a[i]` (in `xmm1`) and then accumulates the result in `xmm0`. There is a data dependency over `xmm0`: a processor cannot issue a new `vfmadd231ss` instruction until the previous one has finished since `xmm0` is both an input and an output of `vfmadd231ss`. Even though multiplication parts of FMA do not depend on each other, these instructions need to wait until all inputs become available. The performance of this loop is bound by FMA latency, which in Intel's Alder Lake is 4 cycles.

In this case, fusing multiplication and addition hurts performance. We would be better off with two separate instructions. The `nanobench` experiment below proves that:

```
# ran on Intel Core i7-1260P (Alder Lake)
$ sudo ./kernel-nanoBench.sh -f -basic │ $ sudo ./kernel-nanoBench.sh -f -basic
 -loop 100 -unroll 1000                │  -loop 100 -unroll 1000 
 -warm_up_count 10 -asm "              │  -warm_up_count 10 -asm "
vmovss xmm1, dword ptr [R14];          │ vmovss xmm1, dword ptr [R14];
vfmadd231ss xmm0, xmm1, xmm1;"         │ vmulss xmm1, xmm1, xmm1;
-asm_init "<not shown>"                │ vaddss xmm0, xmm0, xmm1;"
                                       │ -asm_init "<not shown>"
Instructions retired: 2.00             │ 
Core cycles: 4.00                      │ Instructions retired: 3.00
                                       │ Core cycles: 2.00
```

The version on the left runs in four cycles per iteration, which corresponds to the FMA latency. However, on the right-hand side, `vmulss` instructions do not depend on each other, so they can be run in parallel. Still, there is a loop carry dependency over `xmm0` in the `vaddss` instruction (`FADD`). But the latency of FADD is only two cycles, this is why the version on the right runs in just two cycles per iteration. The latency and throughput characteristics for other processors may vary.[^7]

From this experiment, we know that if the compiler would not have decided to fuse multiplication and addition into a single instruction, it would result in two times better performance for this loop. This only became clear once we examined the loop dependencies and compared the latencies of FMA and FADD instructions. Since Clang 18, you can prevent generating FMA instructions within a scope by using `#pragma clang fp contract(off)`.[^4]

[^1]: Intel APX - [https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html](https://www.intel.com/content/www/us/en/developer/articles/technical/advanced-performance-extensions-apx.html)
[^2]: x86 instruction latency and throughput - [https://uops.info/table.html](https://uops.info/table.html)
[^4]: LLVM extensions to specify floating-point flags - [https://clang.llvm.org/docs/LanguageExtensions.html#extensions-to-specify-floating-point-flags](https://clang.llvm.org/docs/LanguageExtensions.html#extensions-to-specify-floating-point-flags)
[^5]: The debate also isn't interesting because after $\mu$ops conversion, x86 becomes a RISC-style micro-architecture. Complex instructions get broken down into simpler instructions.
[^6]: Also, there are instruction throughput and latency data collected via reverse-engineering experiments, such as in [https://dougallj.github.io/applecpu/firestorm-simd.html](https://dougallj.github.io/applecpu/firestorm-simd.html). Since this is an unofficial source of data, you should take it with a grain of salt.
[^7]: The two versions will produce slightly different results due to the different rounding of floating-point values.
